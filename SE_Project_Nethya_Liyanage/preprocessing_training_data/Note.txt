NOTE: Not all programs I wrote + used are shown here - programs used to determine data contained inside npz/pt files, validation/checking output etc. were not included

All the programs & processes under preprocessing_training_data were carried out by a VM virtual machine.
This was so I could have enough computing power to download, process + refine 21 GB of training data
I used a WSL for some parts, and thus some absolute paths will be in linux

The data I downloaded was (for 15051 proteins)
- FASTA file
- MSAs + Secondary Strucutres (21 GB)
- PDBs

Process was as followed

1. I downloaded the data on the VM
2. I extracted the FASTA file containing sequence of 15051 proteins (extract_tar.py)
3. Refining Using Fasta File
    - I removed similar sequences with 0.7 similarity parameter (misc_cmd_prompts.txt)
    - I removed excessively long files that may lead to higher computational times with a 600 length parameter (len_refinement.py)
    - I randomly removed sequences until there were only 6000 left (random_refinement.py)
4. I extracted a list of the files in the 21 GB folder of MSAs (misc_cmd_prompts.txt)
5. I extracted one single NPZ file to determine what it contained (misc_cmd_prompts.txt)
6. I created a list of the files in the refined list of 6000 fasta sequences (fasta_list.py)
7. I extracted the 6000 npzs associated with the 6000 fasta files in the fasta_list (extract_npz.py) - it took 1.78 hours
8. I extracted the 6000 pdbs assocaited with the 6000 fasta files in the fasta_list (slightly altering the extract_npz.py)
9. Issue - only 5999 files for the npz though there should be 6000. After a lot of checking I found out both folder were missing "3cc2_1_B" - do not know why. need to fix
    - deleting two files (1 pdb + 1 npz) so I don't need to deal with issues
10. I extracted the relevant information from each of the npzs(extract_features_from_npz.py)
11. Preprocessed the ref_npzs & fasta file (normalised, one hot encoded, etc.) to convert to tensors. Used batching to save (preprocessing.py)
12. Merged batched .pt files into final dataset .pt file
13. Padded arrays to same dimensions (pad.py)
    - Dataset became 7G - therefore I removed sequences larger than 350 sequences & padded to 350 (instead of 600)
    - Dataset became 2G
14. Due to some issues training the model, I used plot_dist_ca.py to plot the distance map to determine any errors